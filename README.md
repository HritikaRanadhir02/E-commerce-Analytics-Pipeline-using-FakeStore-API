# E-commerce-Analytics-Pipeline-using-FakeStore-API

E-Commerce Data Pipeline (FakeStore API)

Python | API Extraction | Data Cleaning | Analysis | Automation | SQLite | CSV

ğŸ“Œ 1. Project Overview

This project is an end-to-end data pipeline built using Python to extract, clean, analyze, and store real-time product data from the FakeStore REST API.

It is designed to demonstrate practical skills required for:

-Python Developer

-Data Analyst

-Data Engineer (L1)

-Automation Engineer

This pipeline mimics a mini real-world ETL workflow:

Extract â†’ Transform â†’ Analyze â†’ Load â†’ Automate

The project outputs:

1. A clean CSV file

2. A SQLite database

3. A text-based analysis report

4. Logging + modular code structure for production-like behavior

ğŸ“Œ 2. Why This Project? (Problem Statement)

Modern businesses rely heavily on real-time product analytics:

-E-commerce companies need pricing insights

-Analysts track category performance

-Engineers automate data collection pipelines

-Websites require scraping or API ingestion workflows

However, most beginner projects only show:
âŒ standalone notebooks
âŒ small scripts
âŒ no modular design
âŒ no database storage
âŒ no automation

This project solves that by showcasing a professional pipeline that can scale, be reused, and be extended â€” just like one in a real job.

ğŸ“Œ 3. Project Features
ğŸ”¹ 1. API Data Extraction

Fetches product data using the FakeStore API

Includes retry logic, error handling, and logging

Validates and returns consistent JSON output

ğŸ”¹ 2. Data Transformation

Cleans missing values

Normalizes categories

Flattens nested fields (like rating)

Adds new derived features:

price_with_tax

title_length

category_avg_price

ğŸ”¹ 3. Exploratory Data Analysis

Generates insights such as:

Number of products

Category distribution

Cheapest & costliest items

Highest average price category

Useful for dashboards and reporting

ğŸ”¹ 4. Storage (Load Layer)

Stores results in:

âœ” CSV (products.csv)
âœ” SQLite DB (products.db)

Great for Data Engineering roles.

ğŸ”¹ 5. Modular Code Structure
src/
 â”œâ”€â”€ extract.py      â†’ API extraction
 â”œâ”€â”€ transform.py    â†’ Data cleaning & engineering
 â”œâ”€â”€ analysis.py     â†’ Insight generation
 â””â”€â”€ load.py         â†’ CSV + SQLite storage
main.py              â†’ Pipeline orchestrator

ğŸ”¹ 6. Extensible Architecture

You can easily plug in:

Cron jobs

Airflow

Power BI dashboards

Machine learning models

ğŸ“Œ 4. Tech Stack

Python

Pandas

Requests

SQLite3

Logging

Modular OOP-ready architecture

REST API integration

ğŸ“Œ 5. Dataset Source

Data is sourced from FakeStore API:
https://fakestoreapi.com/products

This provides real-like e-commerce product data including:

Titles

Prices

Categories

Descriptions

Ratings (nested JSON)

ğŸ“Œ 6. Project Architecture
                      +----------------------+
                      |     FakeStore API    |
                      +----------+-----------+
                                 |
                                 â–¼
                      +----------------------+
                      |    Extract Layer     |
                      |  (extract.py)        |
                      +----------------------+
                                 |
                                 â–¼
                      +----------------------+
                      |   Transform Layer    |
                      |  (transform.py)      |
                      +----------------------+
                                 |
                                 â–¼
                      +----------------------+
                      |     Analysis Layer   |
                      |   (analysis.py)      |
                      +----------------------+
                                 |
                                 â–¼
            +----------------------------+-----------------------------+
            |                            |                             |
            â–¼                            â–¼                             â–¼
   +----------------+          +----------------+            +------------------+
   |   CSV Output   |          | SQLite DB      |            | summary.txt      |
   +----------------+          +----------------+            +------------------+

ğŸ“Œ 7. How to Run This Project
ğŸ”§ Step 1: Install Dependencies
pip install requests pandas

ğŸ”§ Step 2: Run the Pipeline
python main.py

ğŸ”§ Step 3: Outputs Created

After running, youâ€™ll see an output/ folder created with:

âœ” products.csv
âœ” products.db
âœ” summary.txt

ğŸ“Œ 8. Sample Output (summary.txt)
Total products: 20
Number of categories: 4
Highest avg price category: Electronics
Cheapest product: Men's Cotton Jacket ($55.99)
Most expensive product: Samsung 49-Inch Monitor ($999.99)

ğŸ“Œ 9. What This Project Demonstrates in Interviews
âœ” Python Developer Skills

API requests

Modular code

Error handling

Logging

âœ” Data Analyst Skills

Data cleaning

Feature engineering

Insights generation

CSV reporting

âœ” Data Engineer Skills

ETL pipeline design

SQLite database load

Folder structure & orchestration

Scalability

âœ” Automation Skills

Pipeline building

Reusable functions

Configurable retry logic

You can confidently talk about this project in ANY tech interview.

ğŸ“Œ 10. Future Enhancements

Integrate scheduling (Cron/Airflow)

Build a Power BI dashboard on top of SQLite

Add product sentiment analysis using scraped reviews

Push data to a cloud warehouse (BigQuery, Snowflake)

Convert pipeline into an API using FastAPI

ğŸ“Œ 11. Author

Hritika Ranadhir
Python | SQL | Data Analyst | Data Engineer
ğŸ“ Pune, India
ğŸ’¼ LinkedIn: linkedin.com/in/hritika-ranadhir
ğŸ’» GitHub: github.com/HritikaRanadhir02
